---
title: "NFL Betting"
output: html_notebook
---

This project aims to build a model that can predict the outcome of an nfl game. Sports betting data is taken from http://spreadspoke.com/, with teams, elo, and games data taken from https://kaggle.com and https://fivethirtyeight.com.

--insert validation set--
--find a way to change the weightings of older observations--
--insert average point differential--
--insert win/loss streak--
--insert recent superbowl winner--

```{r}
setwd("C:/Users/LWW-Dell/Documents/School/HBA2/Data Science/project")
```

```{r}
# loading data cleaning libraries
library(tidyverse)
library(Hmisc)
library(mice)
library(VIM)

# loading model libraries
library(e1071)
library(randomForest)
```

Reading all relevant csv's, will probably add more later.

```{r}
# reading in core nfl csv, teams csv, and elo csv
nfl_df <- read.csv("spreadspoke_scores.csv", header = TRUE, sep = ",")
teams_df <- read.csv("nfl_teams.csv", header = TRUE, sep = ",")
elo_df <- read.csv("nfl_elo.csv", header = TRUE, sep = ",")
```

Removing unecessary features from teams_df, then joining teams_df and changing scoring feature into favorite/not favorite

```{r}
# dropping unnecessary features from teams_df
teams_df <- teams_df[, c(1, 3)]

# merging with nfl_df to map team ids to team names
nfl_df <- merge(nfl_df, teams_df, by.x = "team_home", by.y = "team_name")
nfl_df <- merge(nfl_df, teams_df, by.x = "team_away", by.y = "team_name")

# renaming column names
colnames(nfl_df)[c(18, 19)] <- c("home_id", "away_id")
```

Deleting all rows with blank favourite ID's, serves as a killing two birds with one stone as all games with missing favourite ID's are before 1978. 

```{r}
nfl_df <- subset(nfl_df, team_favorite_id != "")

# replacing all remaining blanks into NA's
nfl_df[nfl_df == ""] <- NA

# test for different year cutoffs
```

Making sure each home/away id matches with each favorite/not favorite id and mapping new scores.

```{r}
# setting factors to characters so as to avoid errors
nfl_df$home_id <- as.character(nfl_df$home_id)
nfl_df$away_id <- as.character(nfl_df$away_id)

# mapping scores to favorite team instead of home/away to create win/loss variable
nfl_df <- mutate(.data = nfl_df, score_fav = if_else(home_id == team_favorite_id, 
                                                     score_home, score_away))
nfl_df <- mutate(.data = nfl_df, score_notfav = if_else(home_id == team_favorite_id, 
                                                     score_away, score_home))

# mapping new scores to fav/notfav teams
nfl_df <- mutate(.data = nfl_df, win_loss = if_else(score_fav > score_notfav, "fav", "underdog"))
```


Adding home/away win variable, editing weather detail variable to specify indoor/outdoor conditions. Any absence of "DOME" is presumed to have been played outdoors with the roof open

```{r}
# setting weather detail to character so as to avoid errors
nfl_df$weather_detail <- as.character(nfl_df$weather_detail)

# need to map out all unique values of weather detail
nfl_df <- mutate(.data = nfl_df, indoor_outdoor = if_else(startsWith(weather_detail, "DOME"), 
                                                          "indoors", "outdoors"))

# deleting original weather detail feature
nfl_df$weather_detail <- NULL
```

Filling in numbers to represent different playoff weeks.

```{r}
nfl_df$schedule_week <- as.character(nfl_df$schedule_week)

nfl_df$schedule_week[nfl_df$schedule_week == "18"] = "17"
nfl_df$schedule_week[nfl_df$schedule_week == "Wildcard"] <- "18"
nfl_df$schedule_week[nfl_df$schedule_week == "Division"] <- "19"
nfl_df$schedule_week[nfl_df$schedule_week == "Conference"] <- "20"
nfl_df$schedule_week[nfl_df$schedule_week == "Superbowl"] <- "21"

nfl_df$schedule_week <- as.numeric(nfl_df$schedule_week)
```

Grouping elo data by different teams over different time periods

```{r}
# cutting out all elo data before 1978
elo_df <- subset(elo_df, season > 1977)

# cutting elo_df into two and grouping by team and year, taking the mean post-elos, qelos, and elo win percentage of each team
elo_df1 <- elo_df[, c(2, 5, 9, 11, 21, 27)]
elo_df2 <- elo_df[, c(2, 6, 10, 12, 22, 28)]

elo_df1 <- subset(elo_df1, is.na(elo1_post) == FALSE)
elo_df2 <- subset(elo_df2, is.na(elo2_post) == FALSE)

elo_df1 <- group_by(elo_df1, season, team1)
elo_df2 <- group_by(elo_df2, season, team2)

elo_df1 <- summarise_at(elo_df1, c(3:6), mean)
elo_df2 <- summarise_at(elo_df2, c(3:6), mean)

# recombining the dataframes and joining onto nfl_df
elo_df <- merge(elo_df1, elo_df2, by.x = c("team1", "season"), by.y = c("team2", "season"))
```

Taking the mean of a feature depending on when a team has been home/away, then joining onto nfl_df by season and team ID

```{r}
# joining onto nfl_df and mapping respective features to fav/underdog teams
elo_df <- elo_df %>%
  rowwise()%>%
  mutate(elo_prob = mean(c(elo_prob1, elo_prob2))) %>%
  mutate(elo = mean(c(elo1_post, elo2_post))) %>%
  mutate(qbelo_prob = mean(c(qbelo_prob1, qbelo_prob2))) %>%
  mutate(qbelo = mean(c(qbelo1_post, qbelo2_post))) 

elo_df <- elo_df[, -c(3:10)]

# joining on only qbelo, all other variables were extremely similar
elo_df <- elo_df[, c(1, 2, 6)]

nfl_df <- merge(nfl_df, elo_df, by.x = c("schedule_season", "home_id"), by.y = c("season", "team1"), all = TRUE)
nfl_df <- merge(nfl_df, elo_df, by.x = c("schedule_season", "away_id"), by.y = c("season", "team1"), all = TRUE)

# changing qbelo column names, then taking qbelo differential and mapping onto fav/underdog teams
colnames(nfl_df)[c(23:24)] <- c("home_qbelo", "away_qbelo")

nfl_df <- nfl_df %>% mutate(fav_qbelo = if_else(home_id == team_favorite_id, home_qbelo, away_qbelo)) %>%
  mutate(underdog_qbelo = if_else(home_id == team_favorite_id, away_qbelo, home_qbelo)) %>%
  mutate(fav_qbelo_diff = fav_qbelo - underdog_qbelo) 
```

Adding the "underdog/favorite home team variable".

```{r}
nfl_df <- mutate(.data = nfl_df, home_fav = if_else(home_id == team_favorite_id, "home_fav", "home_underdog"))
```

Adding winstreak variable.

```{r}

```


Deleting all unecessary features

```{r}
# deleting all rows with NA favorite ID's
nfl_df <- subset(nfl_df, team_favorite_id != "")

# using only 13 features
nfl_df <- nfl_df[, c(1, 7, 8, 12, 13, 15, 16, 17, 18, 21, 22, 27, 28)]
```

Imputing NA values using various methods from the MICE package

```{r}
# plotting missing values
aggr_plot <- aggr(nfl_df, col=c('navyblue','red'), numbers = TRUE, sortVars = TRUE, 
                  labels=names(nfl_df), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```

```{r}
# because over 70% of indoor_outdoor is missing, I will need to delete that column
nfl_df$indoor_outdoor <- NULL
nfl_df$weather_humidity <- NULL

aggr_plot <- aggr(nfl_df, col=c('navyblue','red'), numbers = TRUE, sortVars = TRUE, 
                  labels=names(nfl_df), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

Using the mice package to impute all missing values

```{r}
# create a mice dataset
imputed_data <- mice(data = nfl_df, m = 5, method = "pmm", maxit = 5, seed = 500)

# complete the imputation
nfl_df <- complete(imputed_data, 1)
```

Normalizing, then separating into training/testing

```{r}
# re-arranging and making sure all features are in the correct type
nfl_df <- nfl_df[, c(3, 6, 9, 11, 1, 2, 4, 5, 7, 8, 10)]
```

```{r}
# characterizing and normalizing
for (i in 1:ncol(nfl_df)){
  if (i <= 4){
    nfl_df[, i] = as.character(nfl_df[, i])
  } else {
    nfl_df[, i] = (nfl_df[, i] - mean(nfl_df[, i]))/sd(nfl_df[, i])
  }
}

# converting into dummies for consistency among models
nfl_df <- nfl_df %>%
  mutate(schedule_playoff = if_else(schedule_playoff == "TRUE", 1, 0)) %>%
  mutate(stadium_neutral = if_else(stadium_neutral == "TRUE", 1, 0)) %>%
  mutate(win_loss = if_else(win_loss == "fav", 1, 0)) %>%
  mutate(home_fav = if_else(home_fav == "home_fav", 1, 0))

# training/testing split with 20% being in testing
sample_rate <- 0.8
num_samples <- dim(nfl_df)[1]

training <- sample(1:num_samples, sample_rate*num_samples, replace = FALSE)
training_set <- nfl_df[training, ]
testing_set <- nfl_df[-training, ]
```

Building Models, including a KNN, Randomforest, Decision Tree, Logistic Regression, and SVM

```{r}
svm_model <- svm(win_loss ~., data = training_set, kernel = "radial", cost = 5)
forest_model <- randomForest(win_loss ~., data = training_set, ntree = 120 )
log_model <- glm(win_loss ~., data = training_set, family = binomial(logit))
```

Making Predictions

```{r}
svm_predictions <- predict(svm_model, testing_set, type = "response")
svm_predictions <- round(svm_predictions)

forest_predictions <- predict(forest_model, testing_set, type = "response")
forest_predictions <- round(forest_predictions)

log_predictions <- predict(log_model, testing_set, type = "response")
log_predictions <- round(log_predictions)
```

Evaluating errors

```{r}
wrong <- sum(svm_predictions != testing_set$win_loss)
misclassification_rate <- wrong/nrow(testing_set)
print(misclassification_rate)

wrong <- sum(forest_predictions != testing_set$win_loss)
misclassification_rate <- wrong/nrow(testing_set)
print(misclassification_rate)

wrong <- sum(log_predictions != testing_set$win_loss)
misclassification_rate <- wrong/nrow(testing_set)
print(misclassification_rate)
```

False positive and false negative rates

```{r}
false_positive <- sum(svm_predictions == 1 & testing_set$win_loss == 0)
false_negative <- sum(svm_predictions == 0 & testing_set$win_loss == 1)

false_positive_rate <- false_positive/nrow(testing_set)
false_negative_rate <- false_negative/nrow(testing_set)

print(false_positive_rate)
print(false_negative_rate)

false_positive <- sum(forest_predictions == 1 & testing_set$win_loss == 0)
false_negative <- sum(forest_predictions == 0 & testing_set$win_loss == 1)

false_positive_rate <- false_positive/nrow(testing_set)
false_negative_rate <- false_negative/nrow(testing_set)

print(false_positive_rate)
print(false_negative_rate)

false_positive <- sum(log_predictions == 1 & testing_set$win_loss == 0)
false_negative <- sum(log_predictions == 0 & testing_set$win_loss == 1)

false_positive_rate <- false_positive/nrow(testing_set)
false_negative_rate <- false_negative/nrow(testing_set)

print(false_positive_rate)
print(false_negative_rate)
```

Building the ensemble model with all three models (logistic regression, svm, and randomforest)

```{r}
# building prediction dataframe
pred_df <- data.frame(svm_predictions, forest_predictions, log_predictions)

# measuring correlation between model predictions
cor(pred_df)
```

Seems the models are approaching, but don't reach the threshold for high correlation of 0.75. A max voting ensemble model will be built as it is the simplest and most easily understandable.

```{r}
# building ensemble predictions that performs a majority vote 
ensemble_predictions <- pred_df %>%
  rowwise() %>%
  transmute(en_pred = if_else(sum(svm_predictions, forest_predictions, log_predictions) >= 2, 
                              1, 0))

# calculating misclassification/FP/FN
wrong <- sum(ensemble_predictions != testing_set$win_loss)
misclassification_rate <- wrong/nrow(testing_set)
print(misclassification_rate)

false_positive <- sum(ensemble_predictions == 1 & testing_set$win_loss == 0)
false_negative <- sum(ensemble_predictions == 0 & testing_set$win_loss == 1)

false_positive_rate <- false_positive/nrow(testing_set)
false_negative_rate <- false_negative/nrow(testing_set)

print(false_positive_rate)
print(false_negative_rate)
```

